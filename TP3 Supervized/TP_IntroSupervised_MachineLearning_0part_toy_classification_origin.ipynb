{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TP_IntroSupervised_MachineLearning_0part_toy_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGmKWnrQ83eO"
      },
      "source": [
        "**Toy examples**\n",
        "\n",
        "In this part of the practical session, you will play with some toy data to better understand the classification algorithms seen this morning.\n",
        "\n",
        "Please answer all questions \n",
        "\n",
        "**Deadline**: Upload this notebook, the one about Emotion Recognition and the answers to the theoretical questions to E-Campus. Please verify the exact deadline on E-Campus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9EIv2NP4rst"
      },
      "source": [
        "Let's first load the needed packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuMpOgDzt53m"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt  # for plots\n",
        "from matplotlib.colors import ListedColormap\n",
        "from matplotlib import rc\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import time\n",
        "\n",
        "%matplotlib inline\n",
        "np.random.seed(seed=666)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TE4VF_s35HK5"
      },
      "source": [
        "Here, we define some functions useful for generating and plotting the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ky0_o1k-aMeP"
      },
      "source": [
        "def gaussian_data_generation(n, mean, cov, noise_std):\n",
        "   # create data which follows a multivariate Gaussian distribution\n",
        "   # a white (Gaussian) noise is then added to the data\n",
        "    \n",
        "    assert cov.shape[0] == cov.shape[1], \"Please use only square covariance matrix\"    \n",
        "    assert len(mean) == cov.shape[0], \"the dimension of the mean should be equal to the dimension of the covariance matrix\"\n",
        "    \n",
        "    X = np.random.multivariate_normal(mean, cov, n) # actual data\n",
        "    X = X + np.random.multivariate_normal(np.zeros(len(mean)), noise_std ** 2 *np.eye(len(mean)), n)  # we add white noise to the data     \n",
        "   \n",
        "    return X\n",
        "\n",
        "def frontiere(f, X, y, step=50):\n",
        "    # decision boundary of classifier f\n",
        "    # construct cmap\n",
        "\n",
        "    min_tot = np.min(X)\n",
        "    max_tot = np.max(X)\n",
        "    delta = (max_tot - min_tot) / step\n",
        "    xx, yy = np.meshgrid(np.arange(min_tot, max_tot, delta),\n",
        "                         np.arange(min_tot, max_tot, delta))\n",
        "    z = np.array([f(vec) for vec in np.c_[xx.ravel(), yy.ravel()]])\n",
        "    z = z.reshape(xx.shape)\n",
        "    labels = np.unique(z)\n",
        "    color_blind_list = sns.color_palette(\"colorblind\", labels.shape[0])\n",
        "    sns.set_palette(color_blind_list)\n",
        "    my_cmap = ListedColormap(color_blind_list)\n",
        "    plt.imshow(z, origin='lower', extent=[min_tot, max_tot, min_tot, max_tot],\n",
        "               interpolation=\"mitchell\", alpha=0.80, cmap=my_cmap)\n",
        "\n",
        "    ax = plt.gca()\n",
        "    cbar = plt.colorbar(ticks=labels)\n",
        "    cbar.ax.set_yticklabels(labels)\n",
        "\n",
        "    k = np.unique(y).shape[0]\n",
        "    color_blind_list = sns.color_palette(\"colorblind\", k)\n",
        "    for i, label in enumerate(y):\n",
        "        plt.scatter(X[i, 0], X[i, 1], c=[color_blind_list[int(y[i])]],\n",
        "                    s=80, marker=symlist[int(label)])\n",
        "    plt.ylim([min_tot, max_tot])\n",
        "    plt.xlim([min_tot, max_tot])\n",
        "    ax.get_yaxis().set_ticks([])\n",
        "    ax.get_xaxis().set_ticks([])\n",
        "    \n",
        "def class_int_round(z, n_class):\n",
        "    # rounding needed to go from real to integer values \n",
        "    output = np.round(z).astype(int)\n",
        "    if isinstance(z, np.ndarray):\n",
        "        j = z < 0\n",
        "        output[j] = 0\n",
        "        k = z > n_class - 1\n",
        "        output[k] = n_class - 1\n",
        "    else:\n",
        "        if output < 0:\n",
        "            output = 0\n",
        "        else:\n",
        "            if output > n_class - 1:\n",
        "                output = n_class - 1\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvG_mxt75O3W"
      },
      "source": [
        "The next function is the one you will use to crete the toy data. You can choose among three scenarios: 2, 3 or 4 classes. Each class is composed of 2D points sampled from a multivariate Gaussian distribution. You can choose the number of samples, average and covariance matrix for each class. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQ0I_DWT19-W"
      },
      "source": [
        "def generate_scenario(n_classes=3):\n",
        "    \n",
        "    if n_classes == 2:\n",
        "        # Example with 2 classes\n",
        "        n_0=80\n",
        "        mean_0 = [0, 0]        \n",
        "        cov_0 = np.array([[1, 0.1], [0.1, 0.9]])\n",
        "        X_0=gaussian_data_generation(n_0, mean_0, cov_0, 0.1)\n",
        "        y_0=np.zeros(n_0)\n",
        "        \n",
        "        n_1=80\n",
        "        mean_1 = [3, 2]\n",
        "        cov_1 = np.array([[0.1, 0], [0, 0.5]])\n",
        "        X_1=gaussian_data_generation(n_1, mean_1, cov_1, 0.1)\n",
        "        y_1=np.ones(n_1)\n",
        "        \n",
        "        X=np.concatenate((X_0,X_1))\n",
        "        y=np.concatenate((y_0,y_1))\n",
        "        \n",
        "    elif n_classes == 3:\n",
        "        # Example with 3 classes\n",
        "        n_0=80\n",
        "        mean_0 = [0, 0]        \n",
        "        cov_0 = np.array([[1, 0.1], [0.1, 0.9]])\n",
        "        X_0=gaussian_data_generation(n_0, mean_0, cov_0, 0.1)\n",
        "        y_0=np.zeros(n_0)\n",
        "        \n",
        "        n_1=80\n",
        "        mean_1 = [2, 2]\n",
        "        cov_1 = np.array([[0.1, 0], [0, 0.5]])\n",
        "        X_1=gaussian_data_generation(n_1, mean_1, cov_1, 0.1)\n",
        "        y_1=np.ones(n_1)\n",
        "        \n",
        "        n_2=80\n",
        "        mean_2 = [3, 3]\n",
        "        cov_2 = np.array([[0.5, 0.1], [0.1, 1]])\n",
        "        X_2=gaussian_data_generation(n_2, mean_2, cov_2, 0.1)\n",
        "        y_2=2*np.ones(n_2)\n",
        "        \n",
        "        X=np.concatenate((X_0,X_1,X_2))\n",
        "        y=np.concatenate((y_0,y_1,y_2))\n",
        "    elif n_classes == 4:\n",
        "        # Example with 4 classes\n",
        "        n_0=80\n",
        "        mean_0 = [0, 0]        \n",
        "        cov_0 = np.array([[1, 0.1], [0.1, 0.9]])\n",
        "        X_0=gaussian_data_generation(n_0, mean_0, cov_0, 0.1)\n",
        "        y_0=np.zeros(n_0)\n",
        "        \n",
        "        n_1=80\n",
        "        mean_1 = [3, 3]\n",
        "        cov_1 = np.array([[0.1, 0], [0, 0.5]])\n",
        "        X_1=gaussian_data_generation(n_1, mean_1, cov_1, 0.1)\n",
        "        y_1=np.ones(n_1)\n",
        "        \n",
        "        n_2=80\n",
        "        mean_2 = [0, 3]\n",
        "        cov_2 = np.array([[0.5, 0.1], [0.1, 1]])\n",
        "        X_2=gaussian_data_generation(n_2, mean_2, cov_2, 0.1)\n",
        "        y_2=2*np.ones(n_2)\n",
        "        \n",
        "        n_3=80\n",
        "        mean_3 = [3, 0]\n",
        "        cov_3 = np.array([[0.9, 0.15], [0.15, 0.8]])\n",
        "        X_3=gaussian_data_generation(n_3, mean_3, cov_3, 0.1)\n",
        "        y_3=3*np.ones(n_3)\n",
        "        \n",
        "        X=np.concatenate((X_0,X_1,X_2,X_3))\n",
        "        y=np.concatenate((y_0,y_1,y_2,y_3))  \n",
        "\n",
        "    return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIxzsTU76Yfs"
      },
      "source": [
        "Let's choose a scenario and generate some data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlpzSM06uHqU"
      },
      "source": [
        "######## PARAMETER TO CHOOSE THE SCENARIO (number of classes) #######\n",
        "n_classes=3\n",
        "#####################################################################\n",
        "\n",
        "X, y = generate_scenario(n_classes)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1KJXq5H6mWb"
      },
      "source": [
        "Let's plot the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-orQEg7bvGjh"
      },
      "source": [
        "fig1 = plt.figure(figsize=(6, 6))\n",
        "ax = plt.gca()\n",
        "min_tot = np.min(X)\n",
        "max_tot = np.max(X)\n",
        "symlist = ['o', 'p', '*', 's', '+', 'x', 'D', 'v', '-', '^']\n",
        "k = np.unique(y).shape[0]\n",
        "color_blind_list = sns.color_palette(\"colorblind\", k)\n",
        "for i, label in enumerate(y):\n",
        "    plt.scatter(X[i, 0], X[i, 1], c=[color_blind_list[int(y[i])]],\n",
        "                s=80, marker=symlist[int(label)])\n",
        "#ax.get_yaxis().set_ticks([])\n",
        "#ax.get_xaxis().set_ticks([])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bub4hLbV6o_X"
      },
      "source": [
        "As first classifier, we can use a simple linear regression where we  transform in integers the predictions.\n",
        "\n",
        " **Question**: look at the function 'class_int_round'. Explain what it does."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8RFRQcI2JJX"
      },
      "source": [
        "##############################################################################\n",
        "# Naive linear regression on raw observations\n",
        "##############################################################################\n",
        "\n",
        "resolution_param = 150  # 500 for nice plotting, 50 for fast version\n",
        "regr = LinearRegression()\n",
        "regr.fit(X_train, y_train)\n",
        "y_pred_test = class_int_round(regr.predict(X_test), n_classes)\n",
        "\n",
        "# Plotting part\n",
        "fig0 = plt.figure(figsize=(12, 8))\n",
        "title = \"Left out accuracy (regression w./o. dummy variables)\" + \\\n",
        "        \": {:.2f}\".format(accuracy_score(y_test, y_pred_test))\n",
        "plt.title(title)\n",
        "\n",
        "def f(xx):\n",
        "    \"\"\"Classifier\"\"\"\n",
        "    return class_int_round(regr.predict(xx.reshape(1, -1)), n_classes)\n",
        "frontiere(f, X, y, step=resolution_param)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwyYptQI7Dbm"
      },
      "source": [
        "Instead of using this simple strategy, we can also use a *OneHotEncoder*.\n",
        "\n",
        "**Question**: Do you see any difference in the resulting decision boundaries ? Which is the best strategy in your opinion ? Why ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lPNE7xs1641"
      },
      "source": [
        "##############################################################################\n",
        "# Naive linear regression on dummy variables (OneHotEncoder)\n",
        "##############################################################################\n",
        "resolution_param = 150  \n",
        "enc = OneHotEncoder(categories='auto')\n",
        "enc.fit(y_train.reshape(-1, 1))\n",
        "Y = enc.transform(y_train.reshape(-1, 1)).toarray()\n",
        "regr_multi = LinearRegression()\n",
        "regr_multi.fit(X_train, Y)\n",
        "proba_vector_test = regr_multi.predict(X_test)\n",
        "y_pred_test = class_int_round(regr.predict(X_test), n_classes)\n",
        "\n",
        "# performance evaluation on new dataset\n",
        "y_pred_test = np.argmax(proba_vector_test, axis=1)\n",
        "title = \"Left out accuracy (regression with OneHotEncoder)\" + \\\n",
        "        \": {:.2f}\".format(accuracy_score(y_test, y_pred_test))\n",
        "\n",
        "# Plotting part\n",
        "fig1 = plt.figure(figsize=(12, 8))\n",
        "plt.title(title)\n",
        "\n",
        "\n",
        "def f(xx):\n",
        "    \"\"\"Classifier\"\"\"\n",
        "    return np.argmax(regr_multi.predict(xx.reshape(1, -1)))\n",
        "frontiere(f, X, y, step=resolution_param)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVJBmOX-7tsT"
      },
      "source": [
        "Let's use the other strategies seen this morning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4oUp91v2Qq_"
      },
      "source": [
        "##############################################################################\n",
        "# Logistic regression\n",
        "##############################################################################\n",
        "resolution_param = 150  \n",
        "clf = LogisticRegression(solver='lbfgs',multi_class='ovr') # you can also try multi_class='multinomial', \n",
        "clf.fit(X_train, y_train)\n",
        "y_logit_test = clf.predict(X_test)\n",
        "title = \"Left out accuracy (logistic regression) \" + \\\n",
        "        \": {:.2f}\".format(accuracy_score(y_test, y_logit_test))\n",
        "fig2 = plt.figure(figsize=(12, 8))\n",
        "plt.title(title)\n",
        "\n",
        "\n",
        "def f(xx):\n",
        "    \"\"\"Classifier\"\"\"\n",
        "    return int(clf.predict(xx.reshape(1, -1)))\n",
        "frontiere(f, X, y, step=resolution_param)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pStQ5KH92NOe"
      },
      "source": [
        "##############################################################################\n",
        "# LDA\n",
        "##############################################################################\n",
        "resolution_param = 150  \n",
        "clf_LDA = LinearDiscriminantAnalysis()\n",
        "clf_LDA.fit(X_train, y_train)\n",
        "y_LDA_test = clf_LDA.predict(X_test)\n",
        "\n",
        "title = \"Left out accuracy (LDA) \" + \\\n",
        "        \": {:.2f}\".format(accuracy_score(y_test, y_LDA_test))\n",
        "fig2 = plt.figure(figsize=(12, 8))\n",
        "plt.title(title)\n",
        "\n",
        "\n",
        "def f(xx):\n",
        "    \"\"\"Classifier\"\"\"\n",
        "    return int(clf_LDA.predict(xx.reshape(1, -1)))\n",
        "frontiere(f, X, y, step=resolution_param)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN1vtAUd2PKH"
      },
      "source": [
        "##############################################################################\n",
        "# QDA\n",
        "##############################################################################\n",
        "resolution_param = 150 \n",
        "clf_QDA = QuadraticDiscriminantAnalysis()\n",
        "clf_QDA.fit(X_train, y_train)\n",
        "y_QDA_test = clf_QDA.predict(X_test)\n",
        "title = \"Left out accuracy (QDA) \" + \\\n",
        "        \": {:.2f}\".format(accuracy_score(y_test, y_QDA_test))\n",
        "fig2 = plt.figure(figsize=(12, 8))\n",
        "plt.title(title)\n",
        "\n",
        "\n",
        "def f(xx):\n",
        "    \"\"\"Classifier\"\"\"\n",
        "    return int(clf_QDA.predict(xx.reshape(1, -1)))\n",
        "frontiere(f, X, y, step=resolution_param)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyFuOpRe2SVc"
      },
      "source": [
        "##############################################################################\n",
        "# Naive Bayes \n",
        "##############################################################################\n",
        "resolution_param = 150  \n",
        "clf_GNB = GaussianNB()\n",
        "clf_GNB.fit(X_train, y_train)\n",
        "\n",
        "y_test_GNB = clf_GNB.predict(X_test)\n",
        "\n",
        "title = \"Left out accuracy (GNB) \" + \\\n",
        "        \": {:.2f}\".format(accuracy_score(y_test, y_test_GNB))\n",
        "fig2 = plt.figure(figsize=(12, 8))\n",
        "plt.title(title)\n",
        "\n",
        "\n",
        "def f(xx):\n",
        "    \"\"\"Classifier\"\"\"\n",
        "    return int(clf_GNB.predict(xx.reshape(1, -1)))\n",
        "frontiere(f, X, y, step=resolution_param)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7gxAYn82T5-"
      },
      "source": [
        "##############################################################################\n",
        "# KNN\n",
        "##############################################################################\n",
        "resolution_param = 150  \n",
        "clf_KNN = KNeighborsClassifier()\n",
        "clf_KNN.n_neighbors=5\n",
        "\n",
        "clf_KNN.fit(X_train, y_train)\n",
        "y_KNN_test = clf_KNN.predict(X_test)\n",
        "\n",
        "title = \"Left out accuracy (KNN) \" + \\\n",
        "        \": {:.2f}\".format(accuracy_score(y_test, y_KNN_test))\n",
        "fig2 = plt.figure(figsize=(12, 8))\n",
        "plt.title(title)\n",
        "\n",
        "def f(xx):\n",
        "    \"\"\"Classifier\"\"\"\n",
        "    return int(clf_KNN.predict(xx.reshape(1, -1)))\n",
        "frontiere(f, X, y, step=resolution_param)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucUJjC1C8Djb"
      },
      "source": [
        "**Questions**:\n",
        "\n",
        "\n",
        "*   Describe the decision boundaries of the methods. Are all linear ?\n",
        "*   Using the following code, compare the computational time and the test accuracy of the different methods in the three scenarios. Comment the results.\n",
        "* (Optional) If you change the number of samples per class (both training and test), do the results vary ?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuJC50tF8atX"
      },
      "source": [
        "# example using KNN\n",
        "time_start = time.perf_counter()\n",
        "clf_KNN.fit(X_train, y_train)\n",
        "y_KNN_test = clf_KNN.predict(X_test)\n",
        "time_elapsed = (time.perf_counter() - time_start)\n",
        "print('Computational time:', \"%.2f\" %time_elapsed, 's ; Test accuracy KNN', \"%.2f\" %accuracy_score(y_test, y_KNN_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzjcQx0z83ep"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}